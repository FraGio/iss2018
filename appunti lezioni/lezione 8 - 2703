Fatti pull sulle repositories.
Copiati plugins aggiornati nella cartella "dropins" di Eclipse.
Nuovi progetti in iss201Labs: "mbot" ed "mbot.agent".
Passiamo dall'object oriented a sistemi a componenti, perchè i nostri sistemi sono distribuiti.
L'idea è di usare componenti riusabili, essi sono microservizi con struttura "diversa" da quella solita. L'obiettivo è riutilizzare quello che ho già fatto.
Se ho fatto qualcosa che gira (con test anche) io quella cosa non la tocco più, ma la voglio riutilizzare.
L'ereditarietà è qualcosa che nell'OO programming mi permette di riutilizzare gli oggetti, qui non è detto che ce l'abbia a disposizione.
E' stato inserito un nuovo capitolo (9) nel caseStudy.pdf.
Il prof vuole discutere sul modello della lampadina, sulla struttura di Unity, realizzare un servizio RESTful e vuole passare dal mondo virtuale a quello reale. In quest'ultimo caso devo considerare che sono in un ambiente completamente diverso da quello di Unity.

*** richiesta per Pasqua: pensare a come adattare il codice che va nel mondo virtuale al robot presente nel mondo reale ***

La lampadina non è una lampadina vera e propria, è una THING, un componente che è comandabile da remoto tramite un protocollo (Zigby?).
Il gateway/access point è un middleware nel nostro mondo, questo perchè voglio accendere la lampadina tramite un dispositivo Android. Fa da ponte tra mondo di internet e lampadina.

L'obiettivo del caseStudy2 è di costruire una applicazione come ci pare che sia in grado di comunicare con la lampadina (accendere, spegnere, cambiare luminosità...).

Creiamo un workspace vuoto. Creiamo un progetto nuovo chiamato "it.unibo.mbot". Importiamo il progetto "it.unibo.lib2018". Copiamo il file "mbotExecutor.qa" dal progetto "mbot" del prof. Apriamo il file .qa e diciamo "sì" alla conversione del progetto. Apriamo poi il file .gradle generato automaticamente. Dobbiamo far agganciare le dipendenze 2018, quindi a metà file gradle sostituiamo "iss.libs" con "libs2018". Eseguire primo comando gradle per eclipse, poi rendere "srcMore" cartella "source", togliere come cartella "source" la cartella "src". Nelle proprietà del progetto "mbot" devono esserci parecchie librerie, se non ci sono rieseguire il comando gradle.
Vado nel progetto "mbot" del prof, copio dal file gradle la sezione "ADDED FOR ARDUINO" e la incollo nel file gradle del mio progetto, poi rieseguo il comando gradle.

Leggiamo il modello "mbotExecutor.qa", che è descritto nel caseStudy. Il robot interagisce a scambio di messaggi, lo scopo è eseguire comandi che qualcuno gli manda, sia nel mondo fisico che virtuale (robot avatar). Chi è che manda comandi al robot? O un uomo o una macchina.
Guardando il Qactor "rover":
- Plan waitForCmd[  ] ---> sto aspettando un messaggio
- C'è il dispatch che lui si aspetta "whenMsg moveRover   -> execMove " che si chiama "moveRover". Deve avere un payload, una stringa, che deve contenere info.
   definizione dispatch: "Dispatch moveRover : cmd( CMD )"

Quando arriva un messaggio ("onMsg") il robot cambia stato. Non si muove il robot subito, fa da interprete del messaggio ricevuto. Se non riesco a fare matching con nessuna delle clausole (dei cmd possibili) allora torno in attesa di altri messaggi. Se invece arriva un comando valido passo a un comando di gestione. Es. viene chiamato javaRun. Perchè lo chiamiamo anzichè un comando non custom?
Es. di cmd "turnLeft", ha due javaRun definiti. Voglio muovere sia il robot virtuale che fisico in maniera esplicita, da qui i due comandi separati (più quello di stop).
Perchè nei cmd "forward" e "backward" ho il comando "moveRobotAndAvatar", mentre nel "left" e "right" no? Perchè ci sono diversi gradi di libertà, nel primo caso verrà eseguito il comando sia per il mondo fisico che virtuale, mentre nel secondo vediamo il comando in maniera più analitica, ma il funzionamento è lo stesso. Quindi è possibile definire più nel dettaglio, o meno, nel modello questi comportamenti, ed è il progettista ad occuparsene. Ovviamente il comportamento finale deve essere il medesimo indipendentemente dal tipo di modello.
C'è qualcuno che setta "unityOn"? Al momento no.
Nella v1 del robot prima si interagiva con eventi, adesso con messaggi.
Potrei generare una app che comunica col robot? Conosciamo indirizzo e porta, conosciamo la struttura del messaggio (documento caseStudy), quindi sì. Abbiamo fatto una cosa simile per il radar.
Copiare dal progetto del prof "it.unibo.utils" nel nostro "it.unibo.mbot/src", salvare e vedere se spariscono errori. Copiare "mbotConnArduino.java" in "src" del nostro progetto "mbot". Copiamo con la stessa tecnica anche "it.unibo.mbot.serial".

*** comprarsi robot ("mbot" che è già fatto) per le vacanze...magari chiedere a sabbio se lo ha ***

Se il modello è eseguibile io riesco ad attivarlo. Come si fa? Da qualche parte ci sarà un main da lanciare. E' in "mainCtxMbotExecutor".
Qual è lo scopo di "usercmdmanager"? Fa il mapping degli eventi, a cui lui presta attenzione, ad un messaggio al rover.
Apriamo il browser su localhost:8080, compare un'interfaccia.
Vogliamo far comparire il tasto per Unity. Apriamo il file html "QActorWebUI.html" in "srcMore/ctxMbotExecutor" e sostituiamo il contenuto con quello presente nel progetto del prof, riaggionando la pagina dovremmo vedere il pulsante Unity.
A livello di modello riusciamo a capire cosa dovrebbe succedere premendo sul tasto "unity"? C'è comunicazione tramite websocket tra browser e web server, ogni volta che viene premuto un bottone viene generato un evento "userCommand". Lo "userCommandManager" riceve un evento dalla GUI e trasforma questo evento in un messaggio.
Quindi quando viene premuto il pulsante di Unity verrà gestito l'evento (onEvent usercmd :  usercmd( robotgui(unityAddr(X)) ) ), dopodichè verrà generato un messaggio che viene intercettato dal QActor "rover" che lo elabora ed effettua le necessarie operazioni. "unityConfig" è un fatto (sintassi Prolog), dice l'indirizzo di una macchina sulla quale unity è in esecuzione, più un file .bat che si occupa di lanciare unity.
Copiamo dal progetto del prof il file "unityStart.bat" nel nostro. Premendo sul bottone "UNITY" dovrebbe lanciare il bat e, di conseguenza, unity. Su linux non va perchè non riesce a lanciare il bat, ma si può aprire manualmente Unity. Utile usare unity con risoluzione alta e grafica migliore possibile. 
*** fix lancio di Unity ***

Voglio fare in modo che tutte le volte che il robot passa di fronte al sonar passi informazioni al radar.
Guardiamo il modello, c'è "sonarDetector". Cosa fa? Aspetta eventi da uno dei sonar (fisici o virtuali) e trasforma i dati che arrivano dai sonar al radar.
Come facciamo ad analizzare e progettare questa cosa?
Se guardo la console del "mainCtxMbotExecutor" vedo che, muovendo il robot su Unity, vengono generati degli eventi. 
Se io rendo il radar standalone (cioè con un suo contensto, si veda parte commentata nel modello .qa).
Se scommentiamo quella riga e facciamo partire anche il radar cosa succede? Il sistema è cambiato, adesso ci sono due contesti differenti, quindi ogni volta viene generato un evento viene gestito e distribuito in maniera opportuna.
Proviamo a muovere il robot su Unity e vediamo che le info dei sonar vengono mostrati sul radar.

*** fare in modo che tutte le volte che il robot si trova sotto un radar si fermi ***
*** quando si trova davanti ad un ostacolo il robot si deve fermare e tornare un po' indietro ***
Per fare questo aggiungo un altro componente al mio sistema, non vado a modificare il codice che già funziona.
Il nuovo componente è "it.unibo.mbot.agent" e quello che deve fare è quanto ci siamo appena detti. Leggere il modello presente in "agent/src" per capire di più.
Ci sono più o meno gli stessi eventi prodotti nella app precedente.
L'agent è in attesa di eventi che arrivano dai sonar. Sarà l'utente a pilotare il robot e a far generare eventi ai sonar.
La primitiva "sendTo" significa che so chi è il rover e invia a lui (che lavora in un contensto standalone) un messaggio.
Nel caso del "realSonar" è un po' diverso, ho in più anche la distanza.
"switchTo [ !? foundObstacle ]  avoidRealObstacle" vuol dire che se si trova un ostacolo bisogna evitarlo, tramite piano "avoidRealObstacle".

*** devo fare un agente che si muove nello spazio (dritto), l'idea è che il robot passi sotto al primo sonar a una certa distanza. Lo scopo è arrivare al secondo sonar e fermarsi alla stessa distanza del primo. Tutto va bene se non c'è un ostacolo nel tragitto, altrimenti se trovo un ostacolo mobile devo essere intelligente e aspettare che si sposti, altrimenti se è un ostacolo fisso devo ??? ***

Voglio costruire un webserver RESTful e comandare il robot con questa tecnologia.

Nell'analisi della lampadina bisognava rispondere ad una sola domanda: "qual è il modello della lampadina?". In questo modo so come interagire con essa. Come interagisco? Con API RESTful.