Pull dalle repo, più copia dei plugin xtext.
Installiamo Mosquitto (mqtt, protocollo per scambio di messaggi). *** se non completo è da completare a casa ***
Apriamo il caseStudy1, ci sono stati cambiamenti sulla sez. 9 più la sezione 10, 11 e 12.
L'idea è passare da un mondo ad oggetti ad un mondo a (micro)servizi, per i quali si intende ogni applicazione raggiungibile via rete. Un microservizio doovrebbe utilizare il principio di Occam, cioè evitare di complicare eccessivamente le cose se non è necessario.
Cosa fa l'analista? Evita di pensare alla parola soluzione, a "io farei". Cosa produce analizzando? E il progettista cosa fa e cosa produce?
L'analisi deve aver prodotto qualcosa di cui il progettista è consapevole. Bisogna passare da un'architettura logica (analista) a un'archittettura di progetto (progettista). Se quella logica introduce dei vincoli il progettista dovrebbe tenerne conto.
La progettazione a microservizi è diversa da quella a oggetti a cui siamo abituati.
Si vuole fare un refactoring sfruttando MQTT, un protocollo che va per la maggiore per far emettere informazioni in rete.
Quando lavoriamo a livello di componenti utilizziamo ancora il message-passing?
Si vuole fare un refactoring del robot e radar a microservizi, oltre che lavorare col robot fisico.
Il robot come interprete di comando deve utilizare scambio di messaggi, non eventi. 
Vediamo il paragrafo 9 del caseStudy1: il rover non fa niente se non aspettare comandi ed eseguirli, è un puro esecutore. Ci dev'essere interazione tra radar e robot.
QActor non è come Java, eventi e messaggi sono concetti primitivi.
Evento: è qualcosa che accade, non ha destinatari specifici.
Messaggio: atto deliberato di comunicazione verso un'altra entità
Io voglio continuare a inviare comandi tramite l'interfaccia grafica, che "sotto" usa gli eventi, ma voglio far arrivare l'info al rover tramite messaggi. Ci vuole quindi un mapping.
"driven": appena qualcosa accade succede qualcos'altro. Indipendentemente dalla volontà dell'entità "driven" essa è pilotata, ad esempio nel caso di arrivo di eventi o messaggi.
"based": l'atto è deliberato, posso rimanere in ascolto di eventi/messaggi come ignorarli. Questo vuol dire che l'entità "based" potrebbe perdere l'evento, mentre per un messaggio di sicuro questo verrà accodato e l'info (in questo caso) non viene persa. Quindi è per questo che per messaggio ho una migliore architettura, non rischio la perdita di informazioni, in più siccome il messaggio è un atto voluto non rischio di essere "bombardato" da informazioni (come nel caso degli eventi).
Bisogna quindi riprogettare il rover perchè lavori a messaggi. Es. suona il campanello e genera un evento, deve esserci un qualcosa che intercetta l'evento e genera un messaggio (che viene accodato), questo è il mapping evento-messaggio.
Tutta l'intelligenza applicativa verrà messa nella "mente", cioè nell'Agent. E' un entità esterna al rover, perchè non voglio andare a modificare il codice del robot di continuo se voglio modificare la logica dietro l'agent. E' l'agent che dice al robot cosa deve fare, il robot esegue.
Eliminata la "switchTo", operazione per fare transizione di stato.
All'interno del meta modello è stata introdotta una piccola sezione "event-driven", cioè sez. eseguita dall'infrastruttura se scatta un evento. L'event handler è definito a livello di entità esterna, non di actor. "EventHandler evh for usercmd -print { forwardEvent rover -m moveRover }": rimango in ascolto di eventi, preparo e invio un messaggio (moveRover) al rover con lo stesso payload dell'evento. Sia usercmd che whenMsg hanno stesso payload.
Nel piano "execMove" vedo che non potrò mai comandare il robot finale ?? lavoro a microservizi ???
Ho una parte che eseguo quando arriva il comando di avvio Unity, ho guardia che controlla se Unity è attivo o no.
Guardiamo la struttura del rover (sempre nel modello roverExecutor.qa): nel piano "waitForCmd" ho una whenEvent prima di una whenMsg.
Cerco di realizzare un vocabolario standard per la applicazione (piano "sendToRadar"), sto emettendo info nello stesso vocabolario che il radar si aspetta.
Piano "showObstacle": non mi arriva la distanza da Unity dall'ostacolo, questa parte serve per comunicare con la "mente" (che ha bisogno della info).
Pag. 27 del caseStudy1: ho la moveRobotAndAvatar, quindi a questa versione, quando uno preme un pulsante si ha l'azione sia nel mondo virtuale che in quello reale. Ovviamente deve esistere il robot fisico, come faccio a sapere se esiste? In mbotExetuor.qa ho commentata la parte "onRaspberry". Nell'init ho "initRasp()".

Chi fa questa domanda: "so che ci sono due robot, quello fisico (bot) e quello virtuale (rover). C'è una sola interfaccia di comando o multiple? Quando premo un pulsante deve partire uno solo o tutti e due?" ? La fa l'analista del problema, se non me l'ha detto il committente è chi analizza il problema che deve superare questo ostacolo.
Con la moveRobotAndAvatar sono legato a solo un robot virtuale e uno reale, se ne aggiungo un altro dovrei cambiare il codice.
Siamo ad un punto fondamentale: divide et impera? Con questo approccio ho più flessibilità ma ho anche molti più problemi di progettazione e gestione. Sta nascendo una polveriera.

Torniamo al caseStudy1: mando comandi ad arduino ed unity. Cosa vuol dire lo "0" finale? Vuol dire che il movimento ha durata infinita, cioè fino a nuovo ordine, ma soprattutto ha una semantica asincrona, cioè mi viene immediatamente restituito il controllo, a differenza di un'operazione che ha una durata > 0 nella quale rimarrò in attesa per un certo tempo.

Introduciamo un componente che riceve info dai sonar e le manda al robot, che è l'agent. Questo agente è un modello che rappresenta un sistema fatto da due nodi (due contesti), di questi uno ??
Dentro al contesto ctxMbotExecutor c'è qualcosa che emette eventi. Io progettista devo fare qualcosa per recuperare questi eventi? No, perchè sto lavorando con l'infrastruttura dei QActor.
Io posso attivare il radar che è standalone e riceve eventi "polar". Il over lavora con un contesto standalone che è il radar, quindi l'infrastruttura creerà automaticamente dei canali di comunicazione tra di essi, quindi se viene generato un evento esso arriva direttamente all'altra entità. Attivo l'agent dopo che ho attivato gli altri due, questo è un sottosistema che si rapporta con il rover.
L'evento "polar" viene propagato ad entrambe le entità radar e agent.
Paragrafo 9.3.1: suppongo che l'agent si attacchi al radar. Adesso il radar fa da elemento di aggregazione tra gli elementi del sistema. Adesso quando l'agent emette "polar" va a tutti, ma il radar è anche capace di sentirlo (il robot no). Questa decisione chi la fa? L'analista del problema. Ma per quale motivo il problema mi porta a questa decisione?
Chi pilota concettualmente il radar, cioè la business logic della applicazione? L'agent.
Tra le due architetture la seconda ha più accoppiamento, ma soprattutto genera più traffico di rete (quindi costa di più).

Il sistema è fatto da una logica applicativa che deve mettere in relazione il robot fisico con quello logico.

Una volta che ho fatto un prototipo che mi convince che l'architettura funziona, faccio un refactoring dove non uso più i QActor ma uso il protocollo mqtt per la comunicazione. Gli eventi vengono emessi con un mqtt server. Prima quando il rover emetteva "polar" scattava l'infrastruttura dei QActor, cioè venivano emessi eventi in locale oppure ai context conosciuti. Adesso l'emissione di info viene fatta verso l'mqtt server, quindi il rover deve conoscere solo questo server, non strettamente le altre entità. Si lavora con PUB/SUB, con topic nel quale si guardano i sottoscrittori per recapitare i messaggi in coda.
Pagina 34: il rover agent e il rover sono entrambi publisher e subscriber sul server mqtt, per cui se il rover invierà "sonarSensor" tramite messaggio al rover agent arriverà un evento "sonarSensor".

Attivazione dell'mqtt server in locale, modalità "verbose". Lo scopo è, quando serve, scordarsi dell'infrastruttura a QActor e utilizzare quella tramite mqtt.
Cosa fa il QActor "roveragent"? Aspetta eventi o messaggi.

"onEvent sonarSensor : sonar(sonar1, DISTANCE) -> {
//fixed sonar (moving ahead or backwards)
println( sonar1 ); //no variable substitution, sorry
delay 500 ; //continue to move to stop sonar events
emit usercmd : usercmd( robotgui(h(low)) )" --- > l'idea è che la "mente" invii un comando che il body "rover" deve ricevere.

Paragrafo 10.2.2: c'è un attore "mindToBody" che farà la trasposizione degli eventi, recapitati a questo attore tramite mqtt (-pubsub) o tramite la GUI che genera l'evento. Sto usando un linguaggio uniforme per parlare al robot. Il mindToBody trasforma gli eventi ricevuti in un evento "mindcmd" che arriverà al server e verrà percepito da qualcuno.
Paragrafo 10.2: ??

roverExecutor.qa: c'è la trasformazione da evento a messaggio.
Ho avviato il radar, il robot (che mi dà la gui web), più unity. Il radar è standalone ma comunica tramite mqtt.
Se premo su "forward" venono generati eventi, poi trasformati in messaggi. Se non attivo l'agente non ho la parte di intelligenza, quando il robot passa davanti al sonar non succede nulla. Devo far partire l'mbotAgent. Ora il robot si è fermato quando è passato davanti al sonar, questo perchè sono transitati dei dati via rete. Viene aggiornato anche il radar, perchè è attaccato all'mqtt server anch'esso.